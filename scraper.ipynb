{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alabama', 'alaska', 'arizona', 'arkansas', 'california', 'colorado', 'connecticut', 'washington-dc', 'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'maine', 'maryland', 'massachusetts', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'nebraska', 'nevada', 'new-hampshire', 'new-jersey', 'new-mexico', 'new-york', 'north-carolina', 'north-dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode-island', 'south-carolina', 'south-dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'west-virginia', 'wisconsin', 'wyoming']\n"
     ]
    }
   ],
   "source": [
    "def states_name_scraper(state,url,):\n",
    "  response = requests.get(url,headers=headers)\n",
    "  soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "  links_div = soup.find('div',id = 'links-container-dt')\n",
    "  if links_div:\n",
    "    state_links = links_div.find_all('a',href = True)\n",
    "    for link in state_links:\n",
    "      state_name = link.get_text(strip = True)\n",
    "      state_url = link['href']\n",
    "      if state_name.endswith(\"Homes\") and state_name[:2].isupper():\n",
    "        a = state_url.strip('/')\n",
    "        state.append(a)\n",
    "  return state\n",
    "\n",
    "state = []\n",
    "url = \"https://www.homes.com\"\n",
    "\n",
    "states = states_name_scraper(state,url)\n",
    "\n",
    "print(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alabama',\n",
       " 'alaska',\n",
       " 'arizona',\n",
       " 'arkansas',\n",
       " 'california',\n",
       " 'colorado',\n",
       " 'connecticut',\n",
       " 'washington-dc',\n",
       " 'delaware',\n",
       " 'florida',\n",
       " 'georgia',\n",
       " 'hawaii',\n",
       " 'idaho',\n",
       " 'illinois',\n",
       " 'indiana',\n",
       " 'iowa',\n",
       " 'kansas',\n",
       " 'kentucky',\n",
       " 'louisiana',\n",
       " 'maine',\n",
       " 'maryland',\n",
       " 'massachusetts',\n",
       " 'michigan',\n",
       " 'minnesota',\n",
       " 'mississippi',\n",
       " 'missouri',\n",
       " 'montana',\n",
       " 'nebraska',\n",
       " 'nevada',\n",
       " 'new-hampshire',\n",
       " 'new-jersey',\n",
       " 'new-mexico',\n",
       " 'new-york',\n",
       " 'north-carolina',\n",
       " 'north-dakota',\n",
       " 'ohio',\n",
       " 'oklahoma',\n",
       " 'oregon',\n",
       " 'pennsylvania',\n",
       " 'rhode-island',\n",
       " 'south-carolina',\n",
       " 'south-dakota',\n",
       " 'tennessee',\n",
       " 'texas',\n",
       " 'utah',\n",
       " 'vermont',\n",
       " 'virginia',\n",
       " 'washington',\n",
       " 'west-virginia',\n",
       " 'wisconsin',\n",
       " 'wyoming']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cards_data(states,max_pages = 18):\n",
    "  main_df = pd.DataFrame()\n",
    "\n",
    "  for page_number in range(1,max_pages+1):\n",
    "    url = f\"https://www.homes.com/{state}/sold/p{page_number}\"\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    cards = soup.find_all('article',class_='search=placard sold-placard')\n",
    "\n",
    "    prices = []\n",
    "    status = []\n",
    "    sqft = []\n",
    "    pricepersquarefeett = []\n",
    "    daysonmarket = []\n",
    "    beds = []\n",
    "    baths = []\n",
    "    yearbuilt = []\n",
    "    streetinfo = []\n",
    "    city = []\n",
    "    zipcode = []\n",
    "    descriptions = []\n",
    "    agentnames = []\n",
    "    agencynames = []\n",
    "\n",
    "    for card in cards:\n",
    "      price_info = card.find('p',class_='price-container').get_text(strip = True)\n",
    "      price = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
